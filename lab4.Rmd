---
title: "Lab 4 - Data Cleaning and Exploration"
author: "Name"
date: "Date"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: inline
---

## Instructions and Overview

For this assignment, you will import the dataset you will be working with throughout the quarter, you will clean the dataset, and then you will examine it further. For some, your datasets may not require any form of cleaning. However, for others, there are small things you can do to the data file to prepare the data for analysis. I will walk you through the data import and cleaning steps. I encourage you to read through these sections even if your dataset does not require cleaning, as they will provide helpful tips as to how to prepare a dataset for analysis. Once we get to the data exploration section, you will be expected to fill in the blank sections of the code, run the code, and respond to the short answer questions. **Please respond to all questions in complete sentences.**

This will be the first lab in which you will be expected to engage in coding. As you work through this lab, you should run each of the code chunks in order. Be sure to reference lab 1 if you cannot remember how to run code chunks. In the "Data Cleaning" section, running the code in order and only once is particularly important as you can overwrite some of your cleaning steps if you run the code more than once or out of order. If this happens, you will need to reimport the relevant datasets. 

Coding prompts will be described outside of each code chunk. Inside each code chunk you will find specific instructions for what to do to fill in the code listed at the top. In most code chunks, I also include a line that can serve as a template for how the code should be filled in. In this line, *df* stands for *data frame* and marks where the name of your data frame should go in the code. *VARIABLE_NAME* refers to a variable in your data frame and marks where a variable should be referenced in the code.

These instructions and templates will have a '#' in front of them. This is known as a comment. We include a comment in order to mark text in the code chunk that should *not* be executed. When you remove the '#' or *uncomment* it, the specific uncommented text will become executable. If you try to remove a comment in front of text that R doesn't recognize and then run the code, you will get an error. Be sure to *not* uncomment the instructions or the template as this is text that R will not recognize. 

For most of the prompts in this lab, I have started the code for you, and left blanks for you to fill in. The lines that you will be filling in are currently commented out. As you work through the lab, you will be filling in the blanks based on variables in your own dataset, uncommenting just the one line in the code chunks that you filled in (according to the directions that I provide at the top of the code chunk), and then running the code.

> In the functions below, you are going to see the following set of characters often: %>% This is known in the Tidyverse packagae as a 'pipe'. A pipe connects different functions into one line of code (sort of like a conjunction in a sentence). You can think of the pipe as saying: "and then..." communicating to R that you are going tell it to do something else after the function we just called.

First, run the following code to load the libraries we are going to be using in this lab. 

```{r}
#Run this code chunk to load the tidyverse package and the lubridate package.

library(tidyverse)
library(lubridate)
```

## Data Import

You can import datasets(CSV files) into RStudio by using the function **read.csv("[PATH_TO_DATE_FILE]")**.  In our previous lab, the COVID-19 dataset was accessed via hosted services, but you can also access data from file paths and working directories in your computer. 

When you use the the read.csv function, R will import the selected dataset as a data frame (reference lab 1 if you do not know what a data frame is). In our previous lab, we stored the COVID 19 dataset in a variable using **<-** so that we could reference the dataset throughout the lab. Today, you will be loading the datasets that we will using as examples, as well as your own dataset.

```{r}
#Run this code chunk to import our example dataset.

cases <- read.csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

#Do not worry about this line of code for now. I will explain why we are doing this a bit later.

cases <- 
  cases %>% 
  mutate(Total.Cases = 
           cases %>% 
           select(starts_with("X")) %>% 
           rowSums()
         ) %>%
  select(Province.State, Country.Region, Total.Cases)
```

Now, load your own dataset. First, edit the line below (specifically, the [PATH_TO_DATE_FILE]), then uncomment it by removing the '#' from the front of the code calling hte read.csv function. Finally, run the code chunk and import the data. If the data set is available online, you can use the URL within the function. If not, I suggest you move it to the 'datasets' folder in your working directory. Once it is loaded, you will be able to see the data frame on the top right planel ('Environment' tab). There's no need for you to edit the "stringsAsFactors = FALSE" portion of the code. But, if you want to know what it is or means, I suggest reading this story [here](https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/).

```{r}
#Uncomment and edit the line below and run the code chunk. Is the CSV file online or in your local working directory? Once you figured that out, edit the "PATH_TO_DATE_FILE" section.

#my_dataset<- read.csv("[PATH_TO_DATE_FILE]", stringsAsFactors = FALSE)
```

## Data Cleaning 

### What is the structure of your dataset?

The function **str()** gives us an overview of the structure of the dataset, including the number of observations, the variable names, and each variable's type.  For instance, check out how we would run str() on the cases dataset by running the code chunk below.

```{r}
#Run this code chunk to see the structure of the 'cases' data frame.

str(cases)
```

Run this function for your own dataset. 

```{r}
#Edit and Uncomment the line, using the former line of code as an example. What is the name of your data frame? HINT: What name did you give it when you stored it? [<-]?)

#str(____)

```

### Are any of the variables in your dataset incorrectly data typed?

In R, the basic data types include: 

* numeric (num): numbers that may contain decimals
* integer (int): whole numbers
* character (chr): characters 
* logical (logi): TRUE/FALSE
* complex: complex numbers

We can check the type of a variable within a data frame as follows:

```{r}
#Run this code chunk to check out the type the Total.Cases variable, in the COVID19 data frame.

#typeof(df$VARIABLE_NAME)

typeof(cases$Province.State)
typeof(cases$Country.Region)
typeof(cases$Total.Cases)
```

For most of your datasets, everything imported as the correct type. However, if is common to find several numeric variables that imported as chrs. This often happens when there are characters like commas, dollar signs, percent signs in the column. We need to strip these characters before converting the variable to numeric. if this is your case, let's remove the characters that are appearing in these column with the gsub function, which replaces a character with another character - in this case with nothing. Edit and uncomment the code chunk below. 

```{r}

#df$VARIABLE_NAME <- gsub(pattern = "UNWANTED CHARACTER", replacement = "", df$VARIABLE_NAME)

```

### Do you need to change the data type of any variables?

```{r}
#Fill using one the following: as.numeric, as.character, as.logical

#df$VARIABLE_NAME <- as.numeric(df$VARIABLE_NAME) 

```

### How are Null values represented in your dataset? 

Null values should appear as a greyed-out and italicized *NA* (Not Available). This communicates to R that this is an empty value or, in other words, that there is not data here. However, if not properly formatted when we import the dataset, you may see Null values appear as:

* "NULL" 
* empty strings ("")
* "NONE"
* "NOT AVAILABLE"
* "N/A"

How do I know if my dataset is not correctly formatted? You may start by exploring the data diccionary to your dataset and studying how the designers coded null values. 

In the cases dataset, we can see that empty data is filled with an empty string "". We know this because empty rows values in the Province.State field are simply blank. We will convert such values to NA values. To do this, we will select all of the values equal to ("==") "" (empty string) in the cases dataset, and convert them to NA. Check out how I do this below:

```{r}
#Run.
is.na(cases) <- cases == ""
cases %>% head(10)
```

Knowing how many NA data points we have, may give us an idea of the quality of certain variables in relation to the rest of the dataset. The easiest way to detect missing information is using the is.na() function. Below, I will use the function to identify all the NA values within the Cases dataset. The output is going to be a TRUE/FALSE table, where TRUE means NA. For simplicity, we are only going to see the first 10 results in the data frame.

```{r}
cases %>% 
is.na()%>% 
head(10)
```

But this is not always helpful if we are dealing with big datasets. So, what if we ask R about the sum of NA values per column (variable)? For the Cases dataset, it would look like this:

```{r}
sum(is.na(cases))
sum(is.na(cases$Province.State))
sum(is.na(cases$Country.Region))
sum(is.na(cases$Total.Cases))
```
... that may take a lot of our time if our dataset contains many variables. Let's use a different function, colSums, to get a similar result (more on colSums, below).

```{r}
colSums(sapply(cases, is.na))

```

That's better. In both cases we see that all the NA values are in the Province.State variable. This makes sense, as the dataset contains information at a subnational level for seven countries. Let's drop all the NA values, and see which countries report at the subnational level. 

```{r}
cases %>% 
  drop_na(Province.State) %>% 
  select(Country.Region) %>% 
  distinct()
```
Why do you think these countries report this way? Explore the list of regions and think, why would France use these regional reporting clasification system but the US does not?

```{r}
cases %>% 
  drop_na(Province.State) %>% 
```

If you cannot find a data diccionary, you can always ask R to lists each of the unique values that appears within that variable by calling the **distinct()** function. We will talk more about this function in this lab, when we talk about Values in a Key Categorical Variable. For now, let's use it to see all the values within the Province.State variable. You will see the NA value at the top, right? 

```{r}
cases %>% select(Province.State) %>% distinct()

```

Now that we have explored the Cases dataset, let's explore and transform those missing values in **your dataset**. Once you identified the variables that you need to transform (if any), complete and run the code chunk below. 

```{r}
#Uncomment the line associate with your data frame, and run the code chunk.

#is.na(df) <- df == "unwanted string"  

#For example, if "NULL" appears in your dataset:
#is.na(df) <- df == "NULL"

```

### Do you have any variables in your dataset that refer to specific dates? 

Dates can be converted to a date format using the lubridate package. This is a package in the Tidyverse that makes it possible to extract specific information (such as month or year) from dates, and to compute with dates. In the data diccionary, check out the format of your date. Is it just a year? Just a month? A year, month, and day? Are there times listed? What order are each of these values listed in? [This link](https://lubridate.tidyverse.org/) offers more information about how to structure date conversions. If the date is just a year, we can leave it as an int. Otherwise, we will nned to convert the date to a date format. 

---

## Data Exploration

At this point in the assignment, we will begin exploring and getting to know your data better. We will be learning a number of functions that are made available through dplyr - a package in the Tidyverse that enables us to manipulate and transform data. The four primary functions we will be working with this week and next through dplyr include:

* select() : select variables
* filter() : return only observations that meet a particular criteria
* group_by() : group observations according to a common value
* summarize() : perform an operation and return a single value

In this lab, we will focus on the first two - select() and filter(). You can think of select() as a tool to reference specific columns (or variables) in a rectangular dataset, and filter() as a tool to reference specific rows (or observations) in a rectangular dataset. 

### What kinds of variables are in the dataset?

To begin with, we are going to look at the variables in your dataset. You can check out the variables in your dataset in a number of ways, but perhaps the easiest way at this point will be to reference the 'Environment' tab in the upper right hand corner of RStudio. Click on the arrow next to your data frame name to see an expanded list of variables in your data frame. 

*Nominal categorical variables* are variables that identify something else. They name or categorize something that exists in the world. Sometimes, nominal categorical variables are obvious. For instance, in the Cases dataset, the country NAME is a nominal categorical variable - referring to the actual country or state. *Sometimes, numbers are considered nominal categorical variables.* For instance, a ZIP code is not a value that we operate on but instead refers to a certain place; it is a nominal categorical variable.

> In some of your datasets, 0s and 1s may refer to 'yes' and 'no' in the dataset. This is another case where numbers refer to categorical variables. Always be sure to check if numbers are a tally or measurement of something or if they are referring to something else. If they are referring to something else, often they are a nominal categorical variable.

List two nominal categorical variables in your dataset. Use **select()** to select these variables in your dataset, and use **head(10)** to limit the display to the first 10 rows.

> Note that you may not be able to list three of each below. This is fine.

```{r}
#Uncomment the last line, and fill the appropriate data frame name and variable names for your own dataset. Run.

#df %>% select(VARIABLE_NAME1, VARIABLE_NAME2, VARIABLE_NAME3) %>% head(10)

#Here is an example using the cases dataset
cases %>% select(Province.State, Country.Region) %>% head(10)

#_____ %>% select(_____, _____, _____) %>% head(10)

```

*Ordinal categorical variables* are categorical variables that can be ranked or placed in a particular order. For instance, 'High', 'Medium', and 'Low' have a particular order.

List three ordinal categorical variables in your dataset. Use **select()** to select these variables in your dataset, and use **head(10)** to limit the display to the first 10 rows. 

> Note that you may not be able to list three of each below. Most of you do not have any ordinal categorical variables in your dataset. This is fine.

```{r}
#Uncomment the last line, and fill the appropriate data frame name and variable names for your own dataset. Run.

#df %>% select(VARIABLE_NAME1, VARIABLE_NAME2, VARIABLE_NAME3) %>% head(10)

#_____ %>% select(_____, _____, _____) %>% head(10)

```

*Discrete numeric variables* are numeric variables that represent something that is countable - the number of students in a classroom, the number pages in a book, the number of beds in a hospital. 

List three discrete numerical variables in your dataset. Use **select()** to select these variables in your dataset, and use **head(10)** to limit the display to the first 10 rows.

> Note that you may not be able to list three of each below. Some of you do not have any discrete numeric variables in your dataset. This is fine.

```{r}
#Uncomment the last line, and fill the appropriate data frame name and variable names for your own dataset. Run.

#df %>% select(VARIABLE_NAME1, VARIABLE_NAME2, VARIABLE_NAME3) %>% head(10)

#_____ %>% select(_____, _____, _____) %>% head(10)

```

*Continuous numeric variables* are variables that would take an infinite amount of time to precisely count. You can think of these as variables in which it is always possible to measure the value more precisely. For instance, time would be considered a continuous numeric variable because time can be measured with infinite amount of specificity - hours > minutes > seconds > milliseconds > microseconds > nanoseconds ... and so on. Ruler measurements are also continuous because they can also be measured with infinite more precision. Both latitude and longitude are continuous numeric variables as we can always measure them with more precision. 

List three continuous numeric variables in your dataset. Use **select()** to select these variables in your dataset, and use **head(10)** to limit the display to the first 10 rows.

> Note that you may not be able to list three of each below. Some of you do not have any continuous numeric variables in your dataset. This is fine.

```{r}
#Uncomment the last line, and fill the appropriate data frame name and variable names for your own dataset. Run.

#df %>% select(VARIABLE_NAME1, VARIABLE_NAME2, VARIABLE_NAME3) %>% head(10)


#_____ %>% select(_____, _____, _____) %>% head(10)
```

### What makes each observation in your dataset unique?

The last time that prof. Lindsay Poirier ran this course, one student ran into some issues when trying to make sense of the values reported in her dataset. The student was working with a dataset documenting counts of arrests in each California county each year according to the age, gender, and race/ethnic group of arrestee. Check out this data below. (Note that this may take a few moments to load.)

**NOTE: 01/21/2021 1:00 PM: The State of California Department of Justice - OpenJustice- website canâ€™t be reached. If this problem persist, jump to the following example.**
```{r}
#Run.

ca_arrests <- read.csv("https://data-openjustice.doj.ca.gov/sites/default/files/dataset/2019-06/OnlineArrestDispoData1980-2018.csv", stringsAsFactors = FALSE)

ca_arrests %>% head()

```

The student noted that there were multiple rows reporting different arrest counts in cases where all of the other variables seemed the same - in the same year, same county for the same gender, race, arrest disposition, and age group.

```{r}
#Run.

ca_arrests %>%
  filter(YEAR == 2001 & COUNTY == "Sacramento County" & GENDER == "Male" & RACE == "White" & ARREST_DISP_CODE == "Released" & AGE_GROUP == "20 to 29")
```

See how above, in 2001 in Sacramento Couty white male individuals age 20 to 29 that were released had both 0 and 1 felony arrests? How was this possible? The only other values in the dataset were counts of arrests. There was nothing else in the dataset that could make each row unique. So was there 1 felony arrest for this group or 0? Prof. Poirier went ahead and emailed OpenJustice - the program that had made the dataset available. She asked the following in an email:

> "I'm writing to ask about some issues [a student] came across while analyzing the dataset. It appears that there are several rows in the dataset that report different numeric values but refer to the same set of categories. For example in the attached image, there are three rows that refer to 1980, Alameda County, Male, Other, Complaint Sought, and 18 to 19, but they all report different values. We were wondering if you could explain what makes these rows distinct so we have a better sense of whether it is appropriate to sum them."

A week later, they responded: "The program that aggregates the raw Arrest Disposition data uses certain variables not present in the output file. This currently creates multiple rows when all other present variables are distinctly filtered for. The Summary Offense Category counts (F_TOTAL, M_TOTAL, etc.) must be summed up for the multiple rows present."

In this case, the student needed to transform the data - adding up the numeric values across rows in which all other categorical values were the same in order to account for other categorical variables that were not present in the public data. They would need to do this so that every set of values reported in the data was reported according to a distinct *observational unit*, or in other words, so that they had a way of uniquely identifying what each row in the dataset referred to.

In starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its *observational unit*. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? Consider our example dataset by running the code below.

```{r}
# Remember that paste() allows you to create strings that concatenate other strings that you provide, along with other values. We separate all of the components of the string we wish to paste together with commas. We went over this lab 1. Run this code chunk.

paste("I have", nrow(cases), "unique ____ represented in my dataset.")

```

Get this statement started for your dataset:

```{r}
#Uncomment the last line and fill your data frame name in nrow. You need only fill in the FIRST blank line with your data frame name. Run the code chunk.

#paste("I have", nrow(_____), "unique _____ represented in my dataset.")
```

To figure out how to fill that second blank in the statement, it is often useful to identify a variable or set of variables that can serve as a unique key for the data. A *unique key* is a variable (or set of variables) that uniquely identifies an observation in the dataset. For example, in the ca_arrests dataset above, the unique key would be a long combination of variables (the year, county, gender, race, arrest disposition, and age group would uniquely identify each row only after we had transformed it). Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to. 

What about the cases dataset? Because there are multiple provinces listed for each country (but not for all countries), we can't rely on country to uniquely identify each row. Because the Province.State field is often blank (in instances where the values are only reported at the country level), we also can't use the province as a unique key. See below:

```{r}
#Run. 

# Count the distinct values in your unique key
n_unique_keys <- 
  cases %>% 
  select(Country.Region) %>% 
  n_distinct()

# Count the rows in your dataset
n_rows <- nrow(cases)

# Make sure these numbers are equal
n_unique_keys == n_rows
```

Because this returns FALSE, it would be incorrect to say "I have nrow(df) many unique countries in my dataset."

Instead, we need to rely on both the country and province to uniquely identify each row. 

```{r}
#Run. 

# Count the distinct values in your unique key
n_unique_keys <- 
  cases %>% 
  select(Country.Region, Province.State) %>% 
  n_distinct()

# Count the rows in your dataset
n_rows <- nrow(cases)

# Make sure these numbers are equal
n_unique_keys == n_rows
```

In this case, every row in the dataset is a province and country pair. In other words, the dataset's observational unit is a province/country. Now you can confidently say:

```{r}
#Run. 

paste("I have", nrow(cases), "unique province/countries represented in my dataset.")
```

What variable or combination of variables makes each observation in your dataset unique? Confirm that you are correct below. 

```{r}
# Uncomment below and count the distinct values in your unique key. Note that you may need to select multiple variables. If so, separate them by a comma in the select() function.
#n_unique_keys <- _____ %>% select(_____) %>% n_distinct()

# Uncomment below and count the rows in your dataset by filling in your data frame name.
#n_rows <- nrow(_____)

# Uncomment below and then run the code chunk to make sure these values are equal.
# n_unique_keys == n_rows
```

What does your unique key refer to? In other words, what is the observational unit of your dataset?

```{r eval=FALSE}
Fill your response here. 
```

Fill in the statement below, and make sure that it makes sense with your data. 

```{r}
#Uncomment the line below and fill in both of the blanks. Run.

#paste("I have", nrow(_____), "unique _____ represented in my dataset.")
```


#### Defining Discrete Observational Units

Anytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaged in a process of defining. If I count the number of students in class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I'm going to define "student," those decisions impact the numbers that I produce. When I change my definition of "student," how I go about tabulating students also changes. 

Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined. Consider the cases dataset. Throughout the world, due to competing geopolitics, different countries draw geographic borders and name countries differently. For instance, while the United Nations and many other countries recognize 'Myanmar' as the official name of the southeast-Asian country (after a ruling military junta declared the name change in 1989), the US and the UK refer to the country as 'Burma', arguing that official country name changes should be based on a democratic process. The borders between Israel, Syria, Lebanon, and Palestine are constantly in dispute as political processes and religious factions clash. Check out this long list of [territorial disputes](https://en.wikipedia.org/wiki/List_of_territorial_disputes_. This [very debate](https://github.com/CSSEGISandData/COVID-19/issues/1062) played out in the GitHub community commenting on the Johns Hopkins dataset over how to represent Palestine in the data. The debate also played out over how to represent Taiwan in the data. See, for example, [this](https://github.com/CSSEGISandData/COVID-19/issues/458), [this](https://github.com/CSSEGISandData/COVID-19/issues/461), [this](https://github.com/CSSEGISandData/COVID-19/issues/474), [this](https://github.com/CSSEGISandData/COVID-19/issues/530), and [this](https://github.com/CSSEGISandData/COVID-19/issues/1253). Read through this set of issues, and try and tell me data is not political! So, amongst all of these political factors, how are countries and provinces defined in the cases dataset?

When the data was first released, the Johns Hopkins team producing this dataset were not following a particular naming standard for countries. Check out [this issue](https://github.com/CSSEGISandData/COVID-19/issues/340) in their GitHub issue queue. Several community members requested that the [ISO 3166-2](https://www.iso.org/iso-3166-country-codes.html) country codes be added to the dataset. ISO 3166-2 country codes are numbers standardized by the International Standards Organization for uniquely referring to each country or subdivision; each code corresponds to a country or region recognized by the United Nations. You can keep track of how this issue is progressing [here](https://github.com/CSSEGISandData/COVID-19/issues/372). In this sense, the definitions of provinces/countries is evolving in the dataset - and notably evolving towards standardized definitions recognized by the UN, which may be different than how others define countries.

How are the observational units in your dataset defined? Note that if you have multiple variable constituting your observational unit, you may select just one to compose your response. For instance, how are dates or counties defined? How are neighborhoods defined How are family types defined in the income security data? Be sure to refer to the data documentation. 

```{r eval=FALSE}
Fill your response here. 
```

Who or what organization manages these definitions? In other words, who gets to decide what counts in this data? 

```{r eval=FALSE}
Fill your response here. 
```

### Values in a Key Categorical Variable

As we saw earlier, **distinct()** lists each of the unique values that appears within that variable. This can be useful for determining how different issues are classified in the data. **n_distinct()** counts the number of distinct values in a variable. This let's us know how many categories we are dealing with. For instance, I can find out the typology -and total number- used to categorize distinct dispositions that result after an arrest, by calling:

```{r}
#Run the following code. 

#df %>% select(VARIABLE_NAME) %>% distinct()
ca_arrests %>% select(ARREST_DISP_CODE) %>% distinct()

#df %>% select(VARIABLE_NAME) %>% n_distinct()
ca_arrests %>% select(ARREST_DISP_CODE) %>% n_distinct()
```

Often, as critical data thinkers, it is our job to take something that seems obvious and to question it as if it were strange. When running the function above, we may ask why values are categorized the way that they are - even if those categories seem obvious at first glance. What stories do these categories tell? What are the social practices that make them 'obvious'? Could we use others? What are the contraints (legal, social, cultural, etc.)?

Choose a categorical variable in your dataset to explore further. Be sure to select a variable in which the values represented in each row are likely to appear more than once. Select that variable and then call distinct() and n_distinct(). 

```{r}
#Uncomment the appropriate lines below, and fill in your data frame and categorical variable name.

#Check the distinct values in the variable
#_____ %>% select(_____) %>% distinct()

#Check the number of distinct values in the variable
#_____ %>% select(_____) %>% n_distinct()
```

Reflect on the system of classification. How are the categories divided? Do any of the categories surprise you? Why? In what ways do the categories reflect a particular cultural moment? Conduct a bit of Web research in order to better understand why they are divided the way that they are. 

```{r eval=FALSE}
Fill your response here. 
```

### Missing Data

As we learned earlier, we can check the number of NAs in each column in your dataset by summing the number number of NAs in each column (varaible) with the function **colSums()**.

```{r}
#Run the following code to see how many NAs are in each column of Cases.

#colSums(sapply(df, is.na))
colSums(sapply(cases, is.na))
```

Check the number of NAs in each variable in your dataset by filling in the blanks in the commented code below.

```{r}
#Uncomment the appropriate lines below, and fill in your data frame. Run.
#colSums(sapply(_____, is.na)) 
```

Let's explore a variable with many NAs. This is going to be the first time we see the function filter(). **filter()** subsets our data to the observations (or rows) that meet a certain criteria. Below, we will filter our data to those observations in which a certain variable is an NA. However, we can filter by a number of criteria; for instance, we can filter to those rows with a variable that:

* equals a particular value: == "VALUE"
* is less than a particular value: < VALUE
* is greater than a particular value : > VALUE
* is less than or equal to a particular value: <= VALUE
* is greater than or equal to a particular value: >= VALUE
* is one of a vector of values: %in% c(VALUE1, VALUE2)

Here is how we filter data to the rows in which a certain variable is an NA. I also call head(30) to display the first 30 rows in the dataset. 

```{r}
#Run the following code to filter to rows with NA values. 

#df %>% filter(is.na(VARIABLE_NAME)) %>% head(10) #We add head(10) to limit our output to the first ten rows

cases %>% filter(is.na(Province.State)) %>% head(30)
```

If your dataset has no columns with NAs just note that below, and move on to 4. Otherwise, fill in the blanks below. 

```{r}
#Uncomment the appropriate lines below, and fill in your data frame and variable. Run.

#_____ %>% filter(is.na(_____)) %>% head(10)
```

Is there anything special about these rows? What hypotheses do you have for why there may be missing data in the variable you selected? 

```{r eval=FALSE}
Fill your response here. 
```

Does the data dictionary confirm your hypothesis? What does it say? If the data dictionary has not provided enough information to confirm this, you can also note this here.
 
```{r eval=FALSE}
Fill your response here. 
```

How might these missing values impact your data analysis? Why might it be important to remember that these values are missing as we move forward?

```{r eval=FALSE}
Fill your response here. 
```
